version: '3.8'

services:
  # 1. The AI Brain (Ollama)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    ports:
      - "11434:11434"
    volumes:
      # CORREGIDO: Usando tu volumen existente para Ollama
      - flowsie_ollama_data:/root/.ollama
    networks:
      - ai_net

  # 2. The Logic/UI (Flowise)
  flowise:
    image: flowiseai/flowise:latest
    container_name: flowise
    restart: always
    ports:
      - "4124:3000"
    volumes:
      # CORREGIDO: Usando tu volumen existente para Flowise
      - flowsie_flowise_data:/root/.flowise
    environment:
      - PORT: 3000
    networks:
      - ai_net
    depends_on:
      - ollama

  # 3. The "Auto-Installer" (Downloads qwen2.5:0.5b automatically)
  ollama-model-loader:
    image: curlimages/curl
    container_name: ollama_model_loader
    restart: on-failure
    networks:
      - ai_net
    depends_on:
      - ollama
    command: >
      sh -c "echo '⏳ Waiting for Ollama to start...' &&
             sleep 10 &&
             echo '⬇️  Requesting model download (qwen2.5:0.5b)...' &&
             curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"qwen2.5:0.5b\"}' &&
             echo '✅ Model download request sent! (Check Ollama logs for progress)'"

  # 4. CAJA DE DESARROLLO PARA BENTOML (SOLO HERRAMIENTAS)
  bentoml_devbox:
    container_name: bentoml_dev_tools
    restart: always
    build:
      context: .
      dockerfile: Dockerfile.bento.dev
    volumes:
      - ./mlflow-projects:/workspace
    networks:
      - ai_net

# Definición de todos los volúmenes que usamos en el stack.
volumes:
  # CORREGIDO: Declaramos los volúmenes como externos.
  # Esto le dice a Docker Compose: "Estos volúmenes ya existen, no los crees, solo úsalos".
  flowsie_ollama_data:
    external: true
  flowsie_flowise_data:
    external: true

# Definición de la red compartida.
networks:
  ai_net:
    driver: bridge
