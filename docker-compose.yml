version: '3.8'

services:
  # 1. The AI Brain (Ollama)
  ollama-agent:
    image: ollama/ollama:latest
    container_name: ollama-agent
    restart: always
    ports:
      - "11434:11434"
    volumes:
      # CORREGIDO: Usando tu volumen externo existente
      - flowsie_ollama_data:/root/.ollama
    networks:
      - ai_net

  # 2. The Logic/UI (Flowise)
  flowise-agent:
    image: flowiseai/flowise:latest
    container_name: flowise-agent
    restart: always
    ports:
      - "4124:3000"
    volumes:
      # CORREGIDO: Usando tu volumen externo existente
      - flowsie_flowise_data:/root/.flowise
    environment:
      - "PORT=3000"
    networks:
      - ai_net
    depends_on:
      - ollama-agent

  # 3. The "Auto-Installer" (Downloads qwen2.5:0.5b automatically)
  ollama-model-loader-agent:
    image: curlimages/curl
    container_name: ollama-model-loader-agent
    restart: on-failure
    networks:
      - ai_net
    depends_on:
      - ollama-agent
    command: >
      sh -c "echo '⏳ Waiting for Ollama to start...' &&
             sleep 10 &&
             echo '⬇️  Requesting model download (qwen2.5:0.5b)...' &&
             curl -X POST http://ollama-agent:11434/api/pull -d '{\"name\": \"qwen2.5:0.5b\"}' &&
             echo '✅ Model download request sent! (Check Ollama logs for progress)'"

  # 4. CAJA DE DESARROLLO PARA BENTOML (SOLO HERRAMIENTAS)
  bentoml_devbox:
    container_name: bentoml_dev_tools
    restart: always
    build:
      context: .
      dockerfile: Dockerfile.bento.dev
    volumes:
      - /data/compose/94/mlflow-projects:/workspace
    networks:
      - ai_net

# Definición de todos los volúmenes que usamos en el stack.
volumes:
  # CORREGIDO: Declaramos tus volúmenes existentes como "externos"
  flowsie_ollama_data:
    external: true
  flowsie_flowise_data:
    external: true

# Definición de la red compartida.
networks:
  ai_net:
    driver: bridge
